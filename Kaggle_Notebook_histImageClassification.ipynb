{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a88b43",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-30T15:32:49.981080Z",
     "iopub.status.busy": "2022-06-30T15:32:49.980582Z",
     "iopub.status.idle": "2022-06-30T15:32:53.181767Z",
     "shell.execute_reply": "2022-06-30T15:32:53.180761Z"
    },
    "papermill": {
     "duration": 3.208472,
     "end_time": "2022-06-30T15:32:53.184204",
     "exception": false,
     "start_time": "2022-06-30T15:32:49.975732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import compute_class_weight\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b61b518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T15:32:53.190808Z",
     "iopub.status.busy": "2022-06-30T15:32:53.190400Z",
     "iopub.status.idle": "2022-06-30T15:32:55.203168Z",
     "shell.execute_reply": "2022-06-30T15:32:55.202234Z"
    },
    "papermill": {
     "duration": 2.01858,
     "end_time": "2022-06-30T15:32:55.205525",
     "exception": false,
     "start_time": "2022-06-30T15:32:53.186945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Initially used in this repo: https://github.com/prakashchhipa/Magnification-Prior-Self-Supervised-Method/blob/main/src/data/prepare_data.py\n",
    "'''\n",
    "def splitPatients():\n",
    "    \n",
    "    root = \"../input/breakhis/BreaKHis_v1/BreaKHis_v1/histology_slides/breast\"\n",
    "    benign_list = ['/benign/SOB/adenosis/','/benign/SOB/fibroadenoma/', '/benign/SOB/phyllodes_tumor/','/benign/SOB/tubular_adenoma/']\n",
    "    malignant_list = ['/malignant/SOB/lobular_carcinoma/', '/malignant/SOB/papillary_carcinoma/', '/malignant/SOB/ductal_carcinoma/', '/malignant/SOB/mucinous_carcinoma/']\n",
    "\n",
    "    count = 0\n",
    "    patient_list = list()\n",
    "\n",
    "    for benign_type_dir in benign_list:\n",
    "        p_dir_path = root + benign_type_dir\n",
    "        for p_id in os.listdir(p_dir_path):\n",
    "            patient_list.append(p_dir_path + p_id)\n",
    "            count +=1\n",
    "\n",
    "    for malignant_type_dir in malignant_list:\n",
    "        p_dir_path = root + malignant_type_dir\n",
    "        for p_id in os.listdir(p_dir_path):\n",
    "            patient_list.append(p_dir_path + p_id)\n",
    "            count +=1\n",
    "    \n",
    "    category = list()\n",
    "    for patient_path in patient_list:\n",
    "        category.append(patient_path.split('/')[-1].split('_')[1])\n",
    "        \n",
    "    trainPat, testPat, trainCategory, testCategory = train_test_split(patient_list,category,stratify=category,test_size=0.20,random_state=0)\n",
    "    trainPat, valPat, trainCategory, valCategory = train_test_split(trainPat,trainCategory,stratify=trainCategory,test_size=0.25,random_state=0)\n",
    "    return trainPat, testPat, valPat\n",
    "\n",
    "'''\n",
    "'''\n",
    "def genDataDF(patientList,mode):\n",
    "    content = list()\n",
    "    \n",
    "    for patPath in patientList:\n",
    "        patientEx = Path(patPath).parts[-1]\n",
    "        category = Path(patPath).parts[-4]\n",
    "    \n",
    "        for imgPath in list(Path(patPath).glob(\"**/*.png\")):\n",
    "            magn = imgPath.parts[-2]\n",
    "            content.append({\n",
    "                \"path\":imgPath,\n",
    "                \"patientEx\":patientEx,\n",
    "                \"magn\":magn,\n",
    "                \"category\":category,\n",
    "                \"portion\":mode\n",
    "            })\n",
    "            \n",
    "        \n",
    "    dataDF = pd.DataFrame.from_dict(content)\n",
    "    return dataDF\n",
    "            \n",
    "        \n",
    "if (not os.path.isfile(\"trainDF.csv\")) and (not os.path.isfile(\"valDF.csv\")) and (not os.path.isfile(\"testDF.csv\")):\n",
    "    trainPat, testPat, valPat = splitPatients()\n",
    "    trainDF = genDataDF(trainPat,\"Train\")\n",
    "    valDF = genDataDF(valPat,\"Validation\")\n",
    "    testDF = genDataDF(testPat,\"Test\")\n",
    "    \n",
    "    trainDF.to_csv(\"trainDF.csv\")\n",
    "    valDF.to_csv(\"valDF.csv\")\n",
    "    testDF.to_csv(\"testDF.csv\")\n",
    "\n",
    "else:\n",
    "    trainDF = pd.read_csv(\"trainDF.csv\",index_col=0)\n",
    "    valDF = pd.read_csv(\"valDF.csv\",index_col=0)\n",
    "    testDF = pd.read_csv(\"testDF.csv\",index_col=0)\n",
    "    \n",
    "classes = ['benign', 'malignant']\n",
    "trainDF['label'] = trainDF.category.apply(lambda x: classes.index(x))\n",
    "valDF['label'] = valDF.category.apply(lambda x: classes.index(x))\n",
    "testDF['label'] = testDF.category.apply(lambda x: classes.index(x))\n",
    "\n",
    "trainDF = trainDF.sample(frac=1).reset_index(drop=True)\n",
    "valDF = valDF.sample(frac=1).reset_index(drop=True)\n",
    "testDF = testDF.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e424f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T15:32:55.211827Z",
     "iopub.status.busy": "2022-06-30T15:32:55.211270Z",
     "iopub.status.idle": "2022-06-30T17:32:10.519115Z",
     "shell.execute_reply": "2022-06-30T17:32:10.517614Z"
    },
    "papermill": {
     "duration": 7155.313571,
     "end_time": "2022-06-30T17:32:10.521425",
     "exception": false,
     "start_time": "2022-06-30T15:32:55.207854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   path           patientEx  \\\n",
      "0     ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_MC_14-16456   \n",
      "1     ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_DC_14-14946   \n",
      "2     ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...    SOB_B_F_14-25197   \n",
      "3     ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_DC_14-13993   \n",
      "4     ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_PC_15-190EF   \n",
      "...                                                 ...                 ...   \n",
      "4805  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_DC_14-20636   \n",
      "4806  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_DC_14-16336   \n",
      "4807  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...    SOB_M_PC_14-9146   \n",
      "4808  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...  SOB_B_F_14-29960AB   \n",
      "4809  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...    SOB_M_DC_14-5695   \n",
      "\n",
      "      magn   category portion  label Augmentation  \n",
      "0     400X  malignant   Train      1         None  \n",
      "1     100X  malignant   Train      1         None  \n",
      "2     100X     benign   Train      0         None  \n",
      "3     400X  malignant   Train      1         None  \n",
      "4      40X  malignant   Train      1         None  \n",
      "...    ...        ...     ...    ...          ...  \n",
      "4805   40X  malignant   Train      1         None  \n",
      "4806  400X  malignant   Train      1         None  \n",
      "4807  400X  malignant   Train      1         None  \n",
      "4808  200X     benign   Train      0         None  \n",
      "4809  200X  malignant   Train      1         None  \n",
      "\n",
      "[4810 rows x 7 columns]\n",
      "                                                    path           patientEx  \\\n",
      "0      ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_MC_14-16456   \n",
      "1      ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_DC_14-14946   \n",
      "2      ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...    SOB_B_F_14-25197   \n",
      "3      ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_DC_14-13993   \n",
      "4      ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_PC_15-190EF   \n",
      "...                                                  ...                 ...   \n",
      "14425  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_DC_14-20636   \n",
      "14426  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...   SOB_M_DC_14-16336   \n",
      "14427  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...    SOB_M_PC_14-9146   \n",
      "14428  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...  SOB_B_F_14-29960AB   \n",
      "14429  ../input/breakhis/BreaKHis_v1/BreaKHis_v1/hist...    SOB_M_DC_14-5695   \n",
      "\n",
      "       magn   category portion  label Augmentation  \n",
      "0      400X  malignant   Train      1           HF  \n",
      "1      100X  malignant   Train      1           HF  \n",
      "2      100X     benign   Train      0           HF  \n",
      "3      400X  malignant   Train      1           HF  \n",
      "4       40X  malignant   Train      1           HF  \n",
      "...     ...        ...     ...    ...          ...  \n",
      "14425   40X  malignant   Train      1         None  \n",
      "14426  400X  malignant   Train      1         None  \n",
      "14427  400X  malignant   Train      1         None  \n",
      "14428  200X     benign   Train      0         None  \n",
      "14429  200X  malignant   Train      1         None  \n",
      "\n",
      "[14430 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6363927944f40b4b5b82deb2981135c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6055, 0.7261], device='cuda:0')\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 1 ---> Training Loss = 3.375 - Training Accuracy 0.8602 -Validation Loss = 2.781 - Validation Accuracy = 0.8776\n",
      "[INFO]       Model saved!\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 2 ---> Training Loss = 2.753 - Training Accuracy 0.889 -Validation Loss = 2.995 - Validation Accuracy = 0.8648\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 3 ---> Training Loss = 2.4 - Training Accuracy 0.9046 -Validation Loss = 2.258 - Validation Accuracy = 0.9299\n",
      "[INFO]       Model saved!\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 4 ---> Training Loss = 2.229 - Training Accuracy 0.9125 -Validation Loss = 2.354 - Validation Accuracy = 0.9392\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 5 ---> Training Loss = 2.084 - Training Accuracy 0.9181 -Validation Loss = 2.421 - Validation Accuracy = 0.9226\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 6 ---> Training Loss = 1.895 - Training Accuracy 0.9265 -Validation Loss = 2.036 - Validation Accuracy = 0.9356\n",
      "[INFO]       Model saved!\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 7 ---> Training Loss = 1.981 - Training Accuracy 0.926 -Validation Loss = 2.254 - Validation Accuracy = 0.9108\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 8 ---> Training Loss = 1.995 - Training Accuracy 0.9227 -Validation Loss = 2.018 - Validation Accuracy = 0.9278\n",
      "[INFO]       Model saved!\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 9 ---> Training Loss = 1.786 - Training Accuracy 0.9305 -Validation Loss = 1.965 - Validation Accuracy = 0.9468\n",
      "[INFO]       Model saved!\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 10 ---> Training Loss = 1.643 - Training Accuracy 0.9377 -Validation Loss = 2.488 - Validation Accuracy = 0.911\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 11 ---> Training Loss = 1.551 - Training Accuracy 0.9422 -Validation Loss = 2.082 - Validation Accuracy = 0.9315\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 12 ---> Training Loss = 1.459 - Training Accuracy 0.9453 -Validation Loss = 2.85 - Validation Accuracy = 0.9008\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 13 ---> Training Loss = 1.396 - Training Accuracy 0.9481 -Validation Loss = 1.617 - Validation Accuracy = 0.9548\n",
      "[INFO]       Model saved!\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 14 ---> Training Loss = 1.275 - Training Accuracy 0.9537 -Validation Loss = 2.101 - Validation Accuracy = 0.9238\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 15 ---> Training Loss = 1.292 - Training Accuracy 0.9553 -Validation Loss = 2.446 - Validation Accuracy = 0.9296\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 16 ---> Training Loss = 1.237 - Training Accuracy 0.9561 -Validation Loss = 2.362 - Validation Accuracy = 0.9323\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 17 ---> Training Loss = 1.146 - Training Accuracy 0.9579 -Validation Loss = 2.411 - Validation Accuracy = 0.9381\n",
      "[INFO]       800/14430 samples have passed...\n",
      "[INFO]       1600/14430 samples have passed...\n",
      "[INFO]       2400/14430 samples have passed...\n",
      "[INFO]       3200/14430 samples have passed...\n",
      "[INFO]       4000/14430 samples have passed...\n",
      "[INFO]       4800/14430 samples have passed...\n",
      "[INFO]       5600/14430 samples have passed...\n",
      "[INFO]       6400/14430 samples have passed...\n",
      "[INFO]       7200/14430 samples have passed...\n",
      "[INFO]       8000/14430 samples have passed...\n",
      "[INFO]       8800/14430 samples have passed...\n",
      "[INFO]       9600/14430 samples have passed...\n",
      "[INFO]       10400/14430 samples have passed...\n",
      "[INFO]       11200/14430 samples have passed...\n",
      "[INFO]       12000/14430 samples have passed...\n",
      "[INFO]       12800/14430 samples have passed...\n",
      "[INFO]       13600/14430 samples have passed...\n",
      "[INFO]       14400/14430 samples have passed...\n",
      "[INFO]       Epoch 18 ---> Training Loss = 1.11 - Training Accuracy 0.9609 -Validation Loss = 2.253 - Validation Accuracy = 0.9303\n",
      "[INFO]   Model trained!\n"
     ]
    }
   ],
   "source": [
    "class BCDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, datasetDF):\n",
    "        self.datasetDF = datasetDF\n",
    "        self.transform =  transforms.Compose([\n",
    "                                transforms.ToPILImage(),\n",
    "                                transforms.Resize([224,224]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.datasetDF)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        node = self.datasetDF.iloc[index]\n",
    "        img = cv2.imread(str(node.path))\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if \"Augmentation\" in self.datasetDF.columns:\n",
    "            if node.Augmentation != \"None\":\n",
    "                img = self.augmentor(img,node.Augmentation)\n",
    "                  \n",
    "        img = self.transform(img)\n",
    "        label = node.label\n",
    "        \n",
    "        return img, np.array(int(label))\n",
    "    \n",
    "    def augmentor(self,image,method):\n",
    "        if method == \"HF\":\n",
    "            return cv2.flip(image, 1)\n",
    "        elif method == \"RR\":\n",
    "            angle = 45\n",
    "            image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "            rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "            result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "            return result\n",
    "        \n",
    "\n",
    "    \n",
    "class DLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLModel, self).__init__()\n",
    "        resnet = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        feHidden = 2048\n",
    "        \n",
    "        self.feFinal = nn.Sequential(\n",
    "            nn.Linear(1280 * 7 * 7, feHidden),\n",
    "            nn.BatchNorm1d(feHidden, momentum=0.01),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(feHidden, feHidden//4),\n",
    "            nn.BatchNorm1d(feHidden//4, momentum=0.01),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(feHidden//4,1)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        batchSize = features.shape[0]\n",
    "        featuresDim = features.shape[1]\n",
    "        features = features.view(batchSize,-1)\n",
    "        out = self.feFinal(features)\n",
    "        \n",
    "        return out\n",
    "\n",
    "'''\n",
    "'''\n",
    "def epochTrain(model,device,trainDataGen,optimizer,criterion,report=100):\n",
    "    model.train()\n",
    "\n",
    "    epochLoss = 0\n",
    "    dataCounter = 0\n",
    "    \n",
    "    yTrueList = list()\n",
    "    yPredList = list()\n",
    "\n",
    "    for batchIndex, (X,y) in enumerate(trainDataGen):\n",
    "        dataCounter += X.size(0)\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(X)\n",
    "        \n",
    "        output = output.view(-1, )\n",
    "        \n",
    "        loss = criterion(output, y.float())\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epochLoss += loss.item() * X.size(0)\n",
    "        \n",
    "        pred = (output >= 0.5).long()\n",
    "        #pred = output.max(1, keepdim=True)[1]  \n",
    "\n",
    "        yTrueList.extend(y)\n",
    "        yPredList.extend(pred)\n",
    "\n",
    "        if (batchIndex + 1) % report == 0:\n",
    "            print(\"[INFO]       {}/{} samples have passed...\".format(dataCounter,len(trainDataGen.dataset)))\n",
    "    \n",
    "    epochLoss = epochLoss/len(trainDataGen)\n",
    "    \n",
    "    yTrue = torch.stack(yTrueList, dim=0)\n",
    "    yPred = torch.stack(yPredList, dim=0)\n",
    "    score = f1_score(yTrue.cpu().data.squeeze().numpy(), yPred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    return epochLoss, score\n",
    "\n",
    "'''\n",
    "'''\n",
    "def epochVal(model,device,criterion,valDataGen):\n",
    "    model.eval()\n",
    "\n",
    "    epochLoss = 0\n",
    "    yTrueList = list()\n",
    "    yPredList = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in valDataGen:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)#.view(-1, )\n",
    "\n",
    "            output = model(X)\n",
    "            output = output.view(-1, )\n",
    "            \n",
    "            loss = criterion(output, y.float())\n",
    "            \n",
    "            epochLoss += loss.item() * X.size(0)\n",
    "            \n",
    "            pred = (output >= 0.5).long()\n",
    "            #pred = output.max(1, keepdim=True)[1]    \n",
    "\n",
    "            yTrueList.extend(y)\n",
    "            yPredList.extend(pred)\n",
    "            \n",
    "    yTrue = torch.stack(yTrueList, dim=0)\n",
    "    yPred = torch.stack(yPredList, dim=0)\n",
    "\n",
    "    score = f1_score(yTrue.cpu().data.squeeze().numpy(), yPred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    epochLoss = epochLoss/len(valDataGen)\n",
    "    return epochLoss, score\n",
    "\n",
    "\n",
    "def passAugmentation(df):\n",
    "    methods = [\"HF\",\"RR\"]\n",
    "    \n",
    "    augDFList = list()\n",
    "    for m in methods:\n",
    "        augDF = df.copy()\n",
    "        augDF[\"Augmentation\"] = m\n",
    "        augDFList.append(augDF)\n",
    "        \n",
    "    augDFList.append(df)\n",
    "    \n",
    "    return pd.concat(augDFList,ignore_index=True)\n",
    "    \n",
    "\n",
    "'''\n",
    "'''\n",
    "def deepModelDevelopment(trainDF,valDF,modelName=\"DeepModel\"):\n",
    "    modelPath = os.path.join(\"./Models\",modelName)\n",
    "    os.makedirs(modelPath,exist_ok=True)\n",
    "    writerPath = os.path.join(modelPath,\"runs\")\n",
    "    trainWriter = SummaryWriter(os.path.join(writerPath,\"train\"))\n",
    "    valWriter = SummaryWriter(os.path.join(writerPath,\"val\"))\n",
    "\n",
    "    trainDF[\"Augmentation\"] = \"None\"\n",
    "    print(trainDF)\n",
    "    trainDF = passAugmentation(trainDF)\n",
    "    print(trainDF)\n",
    "    trainDF = trainDF.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    trainDataset = BCDataset(trainDF)\n",
    "    valDataset = BCDataset(valDF)\n",
    "\n",
    "    trainDataGen = torch.utils.data.DataLoader(trainDataset, batch_size=8, shuffle=True)\n",
    "    valDataGen = torch.utils.data.DataLoader(valDataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = DLModel()\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    weights = compute_class_weight(y=trainDF.label.values, class_weight=\"balanced\", classes=[0,1])    \n",
    "    class_weights = torch.FloatTensor(weights)\n",
    "    class_weights = class_weights.cuda()\n",
    "    print(class_weights)\n",
    "    #params_to_update = []\n",
    "    #for name,param in model.named_parameters():\n",
    "    #    if param.requires_grad == True:\n",
    "    #        params_to_update.append(param)\n",
    "    #        print(\"\\t\",name)\n",
    "            \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    sinceLastBest = 1\n",
    "    minLoss = 99999999\n",
    "    \n",
    "    for epoch in range(1,100+1):\n",
    "        trainLoss, trainScore = epochTrain(model,device,trainDataGen,optimizer,criterion)\n",
    "        valLoss, valScore = epochVal(model,device,criterion,valDataGen)\n",
    "\n",
    "        print(\"[INFO]       Epoch {} ---> Training Loss = {:.4} - Training Accuracy {:.4} -Validation Loss = {:.4} - Validation Accuracy = {:.4}\".format(epoch,trainLoss,trainScore,valLoss,valScore))\n",
    "        \n",
    "        trainWriter.add_scalar(\"Loss\",trainLoss,epoch)\n",
    "        trainWriter.add_scalar(\"Accuracy\",trainScore,epoch)\n",
    "        valWriter.add_scalar(\"Loss\",valLoss,epoch)\n",
    "        valWriter.add_scalar(\"Accuracy\",valScore,epoch)\n",
    "        \n",
    "        sinceLastBest += 1\n",
    "        \n",
    "        if valLoss < minLoss:\n",
    "            print(\"[INFO]       Model saved!\")\n",
    "            torch.save(model.state_dict(), os.path.join(modelPath,\"Model_{}.pt\".format(modelName)))\n",
    "            sinceLastBest = 1\n",
    "            minLoss = valLoss\n",
    "\n",
    "        if sinceLastBest > 5:\n",
    "            break\n",
    "    \n",
    "    print(\"[INFO]   Model trained!\")\n",
    "    trainWriter.flush()\n",
    "    valWriter.flush()\n",
    "    \n",
    "    \n",
    "deepModelDevelopment(trainDF,valDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6a10fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T17:32:10.566608Z",
     "iopub.status.busy": "2022-06-30T17:32:10.566295Z",
     "iopub.status.idle": "2022-06-30T17:32:52.705166Z",
     "shell.execute_reply": "2022-06-30T17:32:52.704166Z"
    },
    "papermill": {
     "duration": 42.184696,
     "end_time": "2022-06-30T17:32:52.728709",
     "exception": false,
     "start_time": "2022-06-30T17:32:10.544013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88       548\n",
      "           1       0.96      0.89      0.93       956\n",
      "\n",
      "    accuracy                           0.91      1504\n",
      "   macro avg       0.90      0.91      0.90      1504\n",
      "weighted avg       0.91      0.91      0.91      1504\n",
      "\n",
      "[[515  33]\n",
      " [105 851]]\n"
     ]
    }
   ],
   "source": [
    "model = DLModel()\n",
    "    \n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "modelState = \"./Models/DeepModel/Model_DeepModel.pt\"\n",
    "model.load_state_dict(torch.load(modelState))\n",
    "model.eval()          \n",
    "\n",
    "testDataset = BCDataset(testDF)\n",
    "testDataGen = torch.utils.data.DataLoader(testDataset, batch_size=16, shuffle=False)\n",
    "\n",
    "yTrueList = list()\n",
    "yPredList = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in testDataGen:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device).view(-1, )\n",
    "        \n",
    "        output = model(X)\n",
    "        \n",
    "        pred = (output >= 0.5).long()\n",
    "\n",
    "        yTrueList.extend(y)\n",
    "        yPredList.extend(pred)\n",
    "\n",
    "yTrue = torch.stack(yTrueList, dim=0)\n",
    "yPred = torch.stack(yPredList, dim=0)\n",
    "\n",
    "print(classification_report(y_pred=yPred.cpu().data.squeeze().numpy(),y_true=yTrue.cpu().data.squeeze().numpy()))\n",
    "print(confusion_matrix(y_pred=yPred.cpu().data.squeeze().numpy(),y_true=yTrue.cpu().data.squeeze().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a028c",
   "metadata": {
    "papermill": {
     "duration": 0.021771,
     "end_time": "2022-06-30T17:32:52.772652",
     "exception": false,
     "start_time": "2022-06-30T17:32:52.750881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7214.461822,
   "end_time": "2022-06-30T17:32:56.398978",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-30T15:32:41.937156",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3f42c41a41c6436c98acb739a7aabe14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5ccfc9f2e6d04f3486c4ad298adda0cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_83189f15d73c41cd94a2873a488e940a",
       "placeholder": "​",
       "style": "IPY_MODEL_3f42c41a41c6436c98acb739a7aabe14",
       "value": " 13.6M/13.6M [00:01&lt;00:00, 12.6MB/s]"
      }
     },
     "71118754d318483ca323fc3a6d0397fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ea418f62ed62449a9433977146864216",
       "max": 14212972.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c24bfbe8570d4e38ae39fdf570b86979",
       "value": 14212972.0
      }
     },
     "83189f15d73c41cd94a2873a488e940a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89d9dfd2979247b7aa354f4510778c7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4cd1a2bae2e404498109159d8b8b40d",
       "placeholder": "​",
       "style": "IPY_MODEL_c9acf4c6fb1446bbae02193ea369f665",
       "value": "100%"
      }
     },
     "c24bfbe8570d4e38ae39fdf570b86979": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c9acf4c6fb1446bbae02193ea369f665": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e4cd1a2bae2e404498109159d8b8b40d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e901aad9fe5f4e8cb308f7e60826bca1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea418f62ed62449a9433977146864216": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f6363927944f40b4b5b82deb2981135c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_89d9dfd2979247b7aa354f4510778c7d",
        "IPY_MODEL_71118754d318483ca323fc3a6d0397fb",
        "IPY_MODEL_5ccfc9f2e6d04f3486c4ad298adda0cf"
       ],
       "layout": "IPY_MODEL_e901aad9fe5f4e8cb308f7e60826bca1"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
